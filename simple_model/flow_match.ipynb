{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mДля выполнения ячеек с \"base (Python 3.8.8)\" требуется пакет ipykernel.\n",
      "\u001b[1;31mВыполните следующую команду, чтобы установить \"ipykernel\" в среде Python. \n",
      "\u001b[1;31mКоманда: \"conda install -n base ipykernel --update-deps --force-reinstall\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "GENERATE = False\n",
    "TRAIN = False\n",
    "\n",
    "savedir = \"models/simple\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "SIZE = 1000000\n",
    "\n",
    "\n",
    "backend = jax.default_backend()\n",
    "print(backend)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=f'{savedir}/training.log', level=logging.INFO, format='%(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mДля выполнения ячеек с \"base (Python 3.8.8)\" требуется пакет ipykernel.\n",
      "\u001b[1;31mВыполните следующую команду, чтобы установить \"ipykernel\" в среде Python. \n",
      "\u001b[1;31mКоманда: \"conda install -n base ipykernel --update-deps --force-reinstall\""
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    dim: int\n",
    "    out_dim: int = 1\n",
    "    w: int = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.w)(x)\n",
    "        x = nn.selu(x)\n",
    "        x = nn.Dense(self.w)(x)\n",
    "        x = nn.selu(x)\n",
    "        x = nn.Dense(self.w)(x)\n",
    "        x = nn.selu(x)\n",
    "        x = nn.Dense(self.out_dim)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1019 00:39:21.639217   25957 cuda_dnn.cc:502] There was an error before creating cudnn handle (500): cudaErrorSymbolNotFound : named symbol not found\n",
      "E1019 00:39:21.639794   25957 cuda_dnn.cc:502] There was an error before creating cudnn handle (500): cudaErrorSymbolNotFound : named symbol not found\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jax\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:5548\u001b[0m, in \u001b[0;36mones\u001b[0;34m(shape, dtype, device)\u001b[0m\n\u001b[1;32m   5546\u001b[0m shape \u001b[38;5;241m=\u001b[39m canonicalize_shape(shape)\n\u001b[1;32m   5547\u001b[0m dtypes\u001b[38;5;241m.\u001b[39mcheck_user_dtype_supported(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mones\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 5548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mfull(shape, \u001b[38;5;241m1\u001b[39m, _jnp_dtype(dtype), sharding\u001b[38;5;241m=\u001b[39m_normalize_to_sharding(device))\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:1516\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, sharding)\u001b[0m\n\u001b[1;32m   1514\u001b[0m weak_type \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mis_weakly_typed(fill_value)\n\u001b[1;32m   1515\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype \u001b[38;5;129;01mor\u001b[39;00m _dtype(fill_value))\n\u001b[0;32m-> 1516\u001b[0m fill_value \u001b[38;5;241m=\u001b[39m _convert_element_type(fill_value, dtype, weak_type)\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;66;03m# In tracing mode we can't set sharing explictly and PmapShardng is not\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;66;03m# supported.\u001b[39;00m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;66;03m# NB: Consider using with_sharding_constraint in jitted computation\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;66;03m# if needed?\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sharding, PmapSharding) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(fill_value, array\u001b[38;5;241m.\u001b[39mArrayImpl)):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:585\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type, sharding)\u001b[0m\n\u001b[1;32m    583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m operand\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m convert_element_type_p\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    586\u001b[0m       operand, new_dtype\u001b[38;5;241m=\u001b[39mnew_dtype, weak_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(weak_type),\n\u001b[1;32m    587\u001b[0m       sharding\u001b[38;5;241m=\u001b[39msharding)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:2865\u001b[0m, in \u001b[0;36m_convert_element_type_bind\u001b[0;34m(operand, new_dtype, weak_type, sharding)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_element_type_bind\u001b[39m(operand, \u001b[38;5;241m*\u001b[39m, new_dtype, weak_type, sharding):\n\u001b[0;32m-> 2865\u001b[0m   operand \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mPrimitive\u001b[38;5;241m.\u001b[39mbind(convert_element_type_p, operand,\n\u001b[1;32m   2866\u001b[0m                                 new_dtype\u001b[38;5;241m=\u001b[39mnew_dtype, weak_type\u001b[38;5;241m=\u001b[39mweak_type,\n\u001b[1;32m   2867\u001b[0m                                 sharding\u001b[38;5;241m=\u001b[39msharding)\n\u001b[1;32m   2868\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2869\u001b[0m     operand \u001b[38;5;241m=\u001b[39m pjit\u001b[38;5;241m.\u001b[39mwith_sharding_constraint(operand, sharding)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:438\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    436\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    437\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:442\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    441\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 442\u001b[0m     out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n\u001b[1;32m    443\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:948\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/dispatch.py:90\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     88\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m   outs \u001b[38;5;241m=\u001b[39m fun(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/compiler.py:267\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    262\u001b[0m         built_c, compile_options\u001b[38;5;241m=\u001b[39moptions, host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    265\u001b[0m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    266\u001b[0m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xc\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "jax.numpy.ones(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1019 00:27:13.255363   23608 cuda_dnn.cc:502] There was an error before creating cudnn handle (500): cudaErrorSymbolNotFound : named symbol not found\n",
      "E1019 00:27:13.255979   23608 cuda_dnn.cc:502] There was an error before creating cudnn handle (500): cudaErrorSymbolNotFound : named symbol not found\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m12\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:5548\u001b[0m, in \u001b[0;36mones\u001b[0;34m(shape, dtype, device)\u001b[0m\n\u001b[1;32m   5546\u001b[0m shape \u001b[38;5;241m=\u001b[39m canonicalize_shape(shape)\n\u001b[1;32m   5547\u001b[0m dtypes\u001b[38;5;241m.\u001b[39mcheck_user_dtype_supported(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mones\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 5548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mfull(shape, \u001b[38;5;241m1\u001b[39m, _jnp_dtype(dtype), sharding\u001b[38;5;241m=\u001b[39m_normalize_to_sharding(device))\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:1516\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, sharding)\u001b[0m\n\u001b[1;32m   1514\u001b[0m weak_type \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mis_weakly_typed(fill_value)\n\u001b[1;32m   1515\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype \u001b[38;5;129;01mor\u001b[39;00m _dtype(fill_value))\n\u001b[0;32m-> 1516\u001b[0m fill_value \u001b[38;5;241m=\u001b[39m _convert_element_type(fill_value, dtype, weak_type)\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;66;03m# In tracing mode we can't set sharing explictly and PmapShardng is not\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;66;03m# supported.\u001b[39;00m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;66;03m# NB: Consider using with_sharding_constraint in jitted computation\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;66;03m# if needed?\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sharding, PmapSharding) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(fill_value, array\u001b[38;5;241m.\u001b[39mArrayImpl)):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:585\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type, sharding)\u001b[0m\n\u001b[1;32m    583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m operand\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m convert_element_type_p\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    586\u001b[0m       operand, new_dtype\u001b[38;5;241m=\u001b[39mnew_dtype, weak_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(weak_type),\n\u001b[1;32m    587\u001b[0m       sharding\u001b[38;5;241m=\u001b[39msharding)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:2865\u001b[0m, in \u001b[0;36m_convert_element_type_bind\u001b[0;34m(operand, new_dtype, weak_type, sharding)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_element_type_bind\u001b[39m(operand, \u001b[38;5;241m*\u001b[39m, new_dtype, weak_type, sharding):\n\u001b[0;32m-> 2865\u001b[0m   operand \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mPrimitive\u001b[38;5;241m.\u001b[39mbind(convert_element_type_p, operand,\n\u001b[1;32m   2866\u001b[0m                                 new_dtype\u001b[38;5;241m=\u001b[39mnew_dtype, weak_type\u001b[38;5;241m=\u001b[39mweak_type,\n\u001b[1;32m   2867\u001b[0m                                 sharding\u001b[38;5;241m=\u001b[39msharding)\n\u001b[1;32m   2868\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2869\u001b[0m     operand \u001b[38;5;241m=\u001b[39m pjit\u001b[38;5;241m.\u001b[39mwith_sharding_constraint(operand, sharding)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:438\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    436\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    437\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:442\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    441\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 442\u001b[0m     out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n\u001b[1;32m    443\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:948\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/dispatch.py:90\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     88\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m   outs \u001b[38;5;241m=\u001b[39m fun(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/compiler.py:267\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    262\u001b[0m         built_c, compile_options\u001b[38;5;241m=\u001b[39moptions, host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    265\u001b[0m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    266\u001b[0m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xc\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "key = jax.numpy.ones(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1019 00:25:21.153652   23134 cuda_dnn.cc:502] There was an error before creating cudnn handle (500): cudaErrorSymbolNotFound : named symbol not found\n",
      "E1019 00:25:21.154205   23134 cuda_dnn.cc:502] There was an error before creating cudnn handle (500): cudaErrorSymbolNotFound : named symbol not found\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m12\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:5548\u001b[0m, in \u001b[0;36mones\u001b[0;34m(shape, dtype, device)\u001b[0m\n",
      "\u001b[1;32m   5546\u001b[0m shape \u001b[38;5;241m=\u001b[39m canonicalize_shape(shape)\n",
      "\u001b[1;32m   5547\u001b[0m dtypes\u001b[38;5;241m.\u001b[39mcheck_user_dtype_supported(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mones\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m-> 5548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mfull(shape, \u001b[38;5;241m1\u001b[39m, _jnp_dtype(dtype), sharding\u001b[38;5;241m=\u001b[39m_normalize_to_sharding(device))\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:1516\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, sharding)\u001b[0m\n",
      "\u001b[1;32m   1514\u001b[0m weak_type \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mis_weakly_typed(fill_value)\n",
      "\u001b[1;32m   1515\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype \u001b[38;5;129;01mor\u001b[39;00m _dtype(fill_value))\n",
      "\u001b[0;32m-> 1516\u001b[0m fill_value \u001b[38;5;241m=\u001b[39m _convert_element_type(fill_value, dtype, weak_type)\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;66;03m# In tracing mode we can't set sharing explictly and PmapShardng is not\u001b[39;00m\n",
      "\u001b[1;32m   1518\u001b[0m \u001b[38;5;66;03m# supported.\u001b[39;00m\n",
      "\u001b[1;32m   1519\u001b[0m \u001b[38;5;66;03m# NB: Consider using with_sharding_constraint in jitted computation\u001b[39;00m\n",
      "\u001b[1;32m   1520\u001b[0m \u001b[38;5;66;03m# if needed?\u001b[39;00m\n",
      "\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sharding, PmapSharding) \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(fill_value, array\u001b[38;5;241m.\u001b[39mArrayImpl)):\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:585\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type, sharding)\u001b[0m\n",
      "\u001b[1;32m    583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m operand\n",
      "\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 585\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m convert_element_type_p\u001b[38;5;241m.\u001b[39mbind(\n",
      "\u001b[1;32m    586\u001b[0m       operand, new_dtype\u001b[38;5;241m=\u001b[39mnew_dtype, weak_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(weak_type),\n",
      "\u001b[1;32m    587\u001b[0m       sharding\u001b[38;5;241m=\u001b[39msharding)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/lax/lax.py:2865\u001b[0m, in \u001b[0;36m_convert_element_type_bind\u001b[0;34m(operand, new_dtype, weak_type, sharding)\u001b[0m\n",
      "\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_element_type_bind\u001b[39m(operand, \u001b[38;5;241m*\u001b[39m, new_dtype, weak_type, sharding):\n",
      "\u001b[0;32m-> 2865\u001b[0m   operand \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mPrimitive\u001b[38;5;241m.\u001b[39mbind(convert_element_type_p, operand,\n",
      "\u001b[1;32m   2866\u001b[0m                                 new_dtype\u001b[38;5;241m=\u001b[39mnew_dtype, weak_type\u001b[38;5;241m=\u001b[39mweak_type,\n",
      "\u001b[1;32m   2867\u001b[0m                                 sharding\u001b[38;5;241m=\u001b[39msharding)\n",
      "\u001b[1;32m   2868\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   2869\u001b[0m     operand \u001b[38;5;241m=\u001b[39m pjit\u001b[38;5;241m.\u001b[39mwith_sharding_constraint(operand, sharding)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:438\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n",
      "\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n",
      "\u001b[1;32m    436\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n",
      "\u001b[1;32m    437\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n",
      "\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(find_top_trace(args), args, params)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:442\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n",
      "\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n",
      "\u001b[1;32m    441\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n",
      "\u001b[0;32m--> 442\u001b[0m     out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n",
      "\u001b[1;32m    443\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/core.py:948\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n",
      "\u001b[1;32m    946\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 948\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/dispatch.py:90\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n",
      "\u001b[1;32m     88\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m---> 90\u001b[0m   outs \u001b[38;5;241m=\u001b[39m fun(\u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     92\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/compiler.py:267\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n",
      "\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(\n",
      "\u001b[1;32m    262\u001b[0m         built_c, compile_options\u001b[38;5;241m=\u001b[39moptions, host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks\n",
      "\u001b[1;32m    263\u001b[0m     )\n",
      "\u001b[1;32m    264\u001b[0m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n",
      "\u001b[1;32m    265\u001b[0m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n",
      "\u001b[1;32m    266\u001b[0m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n",
      "\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions)\n",
      "\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xc\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    269\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "key = jax.numpy.ones(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def predict(params, inputs):\n",
    "    return model.apply({\"params\": params}, inputs)\n",
    "\n",
    "@jax.jit\n",
    "def sample_conditional_pt(x0, x1, t, sigma):\n",
    "    t = t.reshape(-1, *([1] * (x0.ndim - 1)))\n",
    "    mu_t = t * x1 + (1 - t) * x0\n",
    "    epsilon = jax.random.normal(jax.random.PRNGKey(0), x0.shape)\n",
    "    return mu_t + sigma * epsilon\n",
    "\n",
    "@jax.jit\n",
    "def compute_conditional_vector_field(x0, x1):\n",
    "    return x1 - x0\n",
    "\n",
    "@jax.jit\n",
    "def loss_ffm_function(params, x1, x0, d, e):\n",
    "    \"\"\"Compute loss on mini-batch\"\"\"\n",
    "    x0 = jax.random.normal(jax.random.PRNGKey(0), (x1.shape[0], 6))\n",
    "    t = jax.random.uniform(jax.random.PRNGKey(0), (x0.shape[0],))\n",
    "    xt = sample_conditional_pt(x0, x1, t, sigma=0.01)\n",
    "    ut = compute_conditional_vector_field(x0, x1)\n",
    "    inputs = jnp.concatenate([xt, d, e, t[:, None]], axis=-1)\n",
    "    vt = predict(params, inputs)\n",
    "    loss = jnp.mean((vt - ut) ** 2)\n",
    "    return loss\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "# Data generation\n",
    "SIZE = 10000  # Example size, adjust as needed\n",
    "m = np.random.uniform(size=SIZE)\n",
    "e = np.random.uniform(size=SIZE)\n",
    "noise = np.random.normal(scale=1e-4, size=SIZE)\n",
    "d = np.power(e, 2) * np.power(m, 3) + m * np.exp(-np.abs(0.2 - e)) + noise\n",
    "data = np.array([m, e, d]).T\n",
    "np.save('data.npy', data)\n",
    "\n",
    "# Model definition\n",
    "class MLP(nn.Module):\n",
    "    dim: int\n",
    "    out_dim: int = 1\n",
    "    w: int = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.w)(x)\n",
    "        x = nn.selu(x)\n",
    "        x = nn.Dense(self.w)(x)\n",
    "        x = nn.selu(x)\n",
    "        x = nn.Dense(self.w)(x)\n",
    "        x = nn.selu(x)\n",
    "        x = nn.Dense(self.out_dim)(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(dim=4)\n",
    "\n",
    "@jax.jit\n",
    "def predict(params, inputs):\n",
    "    return model.apply({\"params\": params}, inputs)\n",
    "\n",
    "@jax.jit\n",
    "def sample_conditional_pt(x0, x1, t, sigma):\n",
    "    t = t.reshape(-1, *([1] * (x0.ndim - 1)))\n",
    "    mu_t = t * x1 + (1 - t) * x0\n",
    "    epsilon = jax.random.normal(jax.random.PRNGKey(0), x0.shape)\n",
    "    return mu_t + sigma * epsilon\n",
    "\n",
    "@jax.jit\n",
    "def compute_conditional_vector_field(x0, x1):\n",
    "    return x1 - x0\n",
    "\n",
    "@jax.jit\n",
    "def loss_ffm_function(params, x1, x0, d, e, key):\n",
    "    t = jax.random.uniform(key, (x0.shape[0],))\n",
    "    xt = sample_conditional_pt(x0, x1, t, sigma=0.01)\n",
    "    ut = compute_conditional_vector_field(x0, x1)\n",
    "    inputs = jnp.concatenate([xt, d, e, t[:, None]], axis=-1)\n",
    "    vt = predict(params, inputs)\n",
    "    loss = jnp.mean((vt - ut) ** 2)\n",
    "    return loss\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "# Training setup\n",
    "key = jax.random.PRNGKey(0)\n",
    "batch_size = 512\n",
    "num_epochs = 20000\n",
    "learning_rate = 0.001\n",
    "optimizer = optax.adamw(learning_rate=learning_rate)\n",
    "params = model.init(key, jnp.ones((1, 4)))\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params[\"params\"],\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "# Load data\n",
    "X = jnp.load('data.npy')\n",
    "dataset = jnp.array(X)\n",
    "loader = jax.random.permutation(key, dataset)\n",
    "\n",
    "losses = []\n",
    "\n",
    "start = time.time()\n",
    "for k in tqdm(range(num_epochs)):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    batch_indices = jax.random.choice(subkey, jnp.arange(len(dataset)), (batch_size,))\n",
    "    batch = dataset[batch_indices]\n",
    "    \n",
    "    x0 = jax.random.uniform(subkey, (batch_size, 1))\n",
    "    x1 = batch[:, 0].reshape(-1, 1)\n",
    "    d = batch[:, 2].reshape(-1, 1)\n",
    "    e = batch[:, 1].reshape(-1, 1)\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_ffm_function, has_aux=False)(state.params, x1, x0, d, e, subkey)\n",
    "    state = update_model(state, grads)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if loss < np.min(losses):\n",
    "        print(f\"Loss is less than min loss at iteration {k+1}\")\n",
    "        with open(\"model_best_iter.pkl\", \"wb\") as f:\n",
    "            jax.experimental.optimizers.serialize(state, f)\n",
    "\n",
    "    if (k+1) % 1000 == 0:\n",
    "        end = time.time()\n",
    "        print(f\"{k+1}: loss {loss:0.3f} time {(end - start):0.2f}\")\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    jax.experimental.optimizers.serialize(state, f)\n",
    "print(f\"Model saved to model.pkl\")\n",
    "np.save(\"losses.npy\", np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conditional_pt(x0, x1, t, sigma):\n",
    "    t = t.reshape(-1, *([1] * (x0.dim() - 1)))\n",
    "    mu_t = t * x1 + (1 - t) * x0\n",
    "    epsilon = torch.randn_like(x0)\n",
    "    return mu_t + sigma * epsilon\n",
    "\n",
    "def compute_conditional_vector_field(x0, x1):\n",
    "    return x1 - x0\n",
    "\n",
    "\n",
    "def d_by_m_e(m, e, noise):\n",
    "    return np.power(e,2) * np.power(m,3) + m * np.exp(- np.abs(0.2 - e)) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 1000000\n",
    "\n",
    "if GENERATE:\n",
    "    m_arr = np.zeros((SIZE, 6), dtype='float')\n",
    "    e_arr = np.zeros((SIZE, 25), dtype='float')\n",
    "    d_arr = np.zeros((SIZE, 50), dtype='float')\n",
    "    d_noise_arr = np.zeros((SIZE, 50), dtype='float')\n",
    "\n",
    "    for i in tqdm(range(SIZE)):\n",
    "        constrain = False\n",
    "        while not constrain:\n",
    "            m = np.random.uniform(size=6)\n",
    "            e = np.sort(np.random.uniform(low=1, high=3, size=25))\n",
    "            d = d_by_m_e(m,e).flatten()\n",
    "            constrain = d.shape[0] == 50\n",
    "            \n",
    "        m_arr[i] = m\n",
    "        e_arr[i] = e\n",
    "        d_arr[i] = d\n",
    "        noise = np.concatenate([np.random.normal(0, 2, size=25), np.random.normal(0, 1, size=25)])\n",
    "        d_noise_arr[i] = d + noise\n",
    "        \n",
    "    np.save('data/25_points/m.npy', m_arr)\n",
    "    np.save('data/25_points/e.npy', e_arr)\n",
    "    np.save('data/25_points/d.npy', d_arr)\n",
    "    np.save('data/25_points/d_noise.npy', d_noise_arr)\n",
    "else:\n",
    "    m_arr = torch.tensor(np.load('data/25_points/m.npy'), dtype=torch.float32)\n",
    "    e_arr = torch.tensor(np.load('data/25_points/e.npy'), dtype=torch.float32)\n",
    "    d_arr = torch.tensor(np.load('data/25_points/d.npy'), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    batch_size = 4096\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(m_arr, e_arr, d_arr)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    sigma = 0.1\n",
    "    dim = m_arr[0].shape[0] + e_arr[0].shape[0] + d_arr[0].shape[0]\n",
    "    print(dim)\n",
    "    model = MLP(dim=dim, out_dim=6, w=256,time_varying=True).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    n_iter = 2000\n",
    "\n",
    "    loss_iter = [np.inf]\n",
    "\n",
    "    for k in tqdm(range(n_iter)):\n",
    "        loss_epoch = []\n",
    "        for m, e, d in loader:\n",
    "            optimizer.zero_grad()\n",
    "            # noise = torch.cat([\n",
    "            #     torch.randn(d.shape[0], 4) * 2,\n",
    "            #     torch.randn(d.shape[0], 4) * 1\n",
    "            # ], dim=1)\n",
    "            # d = d + noise\n",
    "            d = d.to(device)\n",
    "            e = e.to(device)\n",
    "            x1 = m.to(device)\n",
    "            x0 = torch.rand(x1.shape[0], 6).to(device)\n",
    "            t = torch.rand(x0.shape[0]).type_as(x0)\n",
    "            xt = sample_conditional_pt(x0, x1, t, sigma=0.01)\n",
    "            ut = compute_conditional_vector_field(x0, x1)\n",
    "            vt = model(torch.cat([xt, d, e, t[:, None]], dim=-1))\n",
    "            loss = torch.mean((vt - ut) ** 2)\n",
    "            \n",
    "            loss_epoch.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if np.mean(loss_epoch) < np.min(loss_iter):\n",
    "            torch.save(model.state_dict(), f\"{savedir}/model_best_epoch.pt\")\n",
    "            logging.info(f\"Model saved to {savedir}/model_best_epoch.pt\")\n",
    "            logging.info(f\"{np.min(loss_iter):0.4f}\")\n",
    "        loss_iter.append(np.mean(loss_epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{savedir}/model.pt\")\n",
    "    logging.info(f\"Model saved to {savedir}/model.pt\")\n",
    "    np.save(f'{savedir}/losses_4.npy', np.array(loss_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = torch.rand(1, 6)\n",
    "m = [0.4, 0.3, 0.3, 0.1, 0.15, 0.6]\n",
    "e = np.linspace(1,3,25)\n",
    "d = d_by_m_e(m ,e).flatten()\n",
    "m = torch.tensor(m,  dtype=torch.float32).unsqueeze(0)\n",
    "e = torch.tensor(e,  dtype=torch.float32).unsqueeze(0)\n",
    "d = torch.tensor(d,  dtype=torch.float32).unsqueeze(0)\n",
    "dim = m[0].shape[0] + e[0].shape[0] + d[0].shape[0]\n",
    "print(dim)\n",
    "model = MLP(dim=dim, out_dim=6, w=256,time_varying=True)\n",
    "model.load_state_dict(torch.load(f\"{savedir}/model.pt\",  map_location='cpu'))\n",
    "\n",
    "def ode_function(t, m, d, e):\n",
    "    t = torch.tensor(t, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    m = torch.tensor(m,  dtype=torch.float32).unsqueeze(0)\n",
    "    return model(torch.cat([m, d, e, t], dim=-1)).detach().numpy()[0]\n",
    "\n",
    "solution = solve_ivp(ode_function, t_span=[0, 1], y0=m0[0], t_eval=None, args=(d, e))\n",
    "d_pred = d_by_m_e(solution.y[:, -1],e[0]).flatten()\n",
    "diff_norm = torch.linalg.norm(d - d_pred) / torch.linalg.norm(d)\n",
    "\n",
    "print(f'm_pred = {solution.y[:, -1]}')\n",
    "print(f'd = {d}')\n",
    "print(f'd_pred = {d_pred}')\n",
    "print(f'diff norm = {diff_norm}')\n",
    "\n",
    "logging.info(f'm_pred = {solution.y[:, -1]}')\n",
    "logging.info(f'd = {d}')\n",
    "logging.info(f'd_pred = {d_pred}')\n",
    "logging.info(f'diff norm = {diff_norm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "sols = []\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    m0 = torch.rand(1, 6)\n",
    "    m = [0.4, 0.3, 0.3, 0.1, 0.15, 0.6]\n",
    "    e = np.linspace(1,3,25)\n",
    "    d = d_by_m_e(m ,e).flatten()\n",
    "    m = torch.tensor(m,  dtype=torch.float32).unsqueeze(0)\n",
    "    e = torch.tensor(e,  dtype=torch.float32).unsqueeze(0)\n",
    "    d = torch.tensor(d,  dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    solution = solve_ivp(ode_function, t_span=[0, 1], y0=m0[0], t_eval=None, args=(d, e))\n",
    "    d_pred = d_by_m_e(solution.y[:, -1],e[0]).flatten()\n",
    "    sols.append(solution.y[:, -1])\n",
    "    errors.append(np.linalg.norm(d - d_pred) / np.linalg.norm(d))\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "df = pd.DataFrame(sols, columns = [fr'$\\beta_1$', fr'$\\alpha', fr'$\\gamma^r$', fr'$\\gamma^d_1$', fr'$\\beta_2$', fr'$\\gamma^d_2$'])\n",
    "\n",
    "plt.figure(dpi=300, figsize=(12,6))\n",
    "sns.kdeplot(df, fill=True, alpha=0.5, common_norm=True)\n",
    "plt.title(fr'Joint probability distribution $\\rho(m|d,e)$', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Parameter value')\n",
    "plt.legend(fontsize=12, loc='best')\n",
    "plt.xlim(0,1)\n",
    "plt.savefig('seir_25p.png')\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(errors), np.std(errors))\n",
    "logging.info(np.mean(errors), np.std(errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
